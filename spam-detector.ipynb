{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c63b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utility functions \n",
    "\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from hashlib import md5\n",
    "\n",
    "maketrans = str.maketrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad198315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All util functions\n",
    "\n",
    "def vectorize_sequences(sequences, vocabulary_length):\n",
    "    results = np.zeros((len(sequences), vocabulary_length))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0\n",
    "    return results\n",
    "\n",
    "\n",
    "def one_hot_encode(messages, vocabulary_length):\n",
    "    data = []\n",
    "    for msg in messages:\n",
    "        temp = one_hot(msg, vocabulary_length)\n",
    "        data.append(temp)\n",
    "    return data\n",
    "\n",
    "def text_to_word_sequence(\n",
    "    text, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \"\n",
    "):\n",
    "    \"\"\"Converts a text to a sequence of words (or tokens).\n",
    "    # Arguments\n",
    "        text: Input text (string).\n",
    "        filters: list (or concatenation) of characters to filter out, such as\n",
    "            punctuation. Default: `!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n`,\n",
    "            includes basic punctuation, tabs, and newlines.\n",
    "        lower: boolean. Whether to convert the input to lowercase.\n",
    "        split: str. Separator for word splitting.\n",
    "    # Returns\n",
    "        A list of words (or tokens).\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    translate_dict = dict((c, split) for c in filters)\n",
    "    translate_map = maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    seq = text.split(split)\n",
    "    return [i for i in seq if i]\n",
    "\n",
    "def one_hot(\n",
    "    text, n, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \"\n",
    "):\n",
    "    \"\"\"One-hot encodes a text into a list of word indexes of size n.\n",
    "    This is a wrapper to the `hashing_trick` function using `hash` as the\n",
    "    hashing function; unicity of word to index mapping non-guaranteed.\n",
    "    # Arguments\n",
    "        text: Input text (string).\n",
    "        n: int. Size of vocabulary.\n",
    "        filters: list (or concatenation) of characters to filter out, such as\n",
    "            punctuation. Default: `!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n`,\n",
    "            includes basic punctuation, tabs, and newlines.\n",
    "        lower: boolean. Whether to set the text to lowercase.\n",
    "        split: str. Separator for word splitting.\n",
    "    # Returns\n",
    "        List of integers in [1, n]. Each integer encodes a word\n",
    "        (unicity non-guaranteed).\n",
    "    \"\"\"\n",
    "    return hashing_trick(\n",
    "        text, n, hash_function=\"md5\", filters=filters, lower=lower, split=split\n",
    "    )\n",
    "\n",
    "\n",
    "def hashing_trick(\n",
    "    text,\n",
    "    n,\n",
    "    hash_function=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "):\n",
    "    \"\"\"Converts a text to a sequence of indexes in a fixed-size hashing space.\n",
    "    # Arguments\n",
    "        text: Input text (string).\n",
    "        n: Dimension of the hashing space.\n",
    "        hash_function: defaults to python `hash` function, can be 'md5' or\n",
    "            any function that takes in input a string and returns a int.\n",
    "            Note that 'hash' is not a stable hashing function, so\n",
    "            it is not consistent across different runs, while 'md5'\n",
    "            is a stable hashing function.\n",
    "        filters: list (or concatenation) of characters to filter out, such as\n",
    "            punctuation. Default: `!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n`,\n",
    "            includes basic punctuation, tabs, and newlines.\n",
    "        lower: boolean. Whether to set the text to lowercase.\n",
    "        split: str. Separator for word splitting.\n",
    "    # Returns\n",
    "        A list of integer word indices (unicity non-guaranteed).\n",
    "    `0` is a reserved index that won't be assigned to any word.\n",
    "    Two or more words may be assigned to the same index, due to possible\n",
    "    collisions by the hashing function.\n",
    "    The [probability](\n",
    "        https://en.wikipedia.org/wiki/Birthday_problem#Probability_table)\n",
    "    of a collision is in relation to the dimension of the hashing space and\n",
    "    the number of distinct objects.\n",
    "    \"\"\"\n",
    "    if hash_function is None:\n",
    "        hash_function = hash\n",
    "    elif hash_function == \"md5\":\n",
    "        hash_function = lambda w: int(md5(w.encode()).hexdigest(), 16)\n",
    "\n",
    "    seq = text_to_word_sequence(text, filters=filters, lower=lower, split=split)\n",
    "    return [int(hash_function(w) % (n - 1) + 1) for w in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ba3675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "from sagemaker.mxnet import MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888dbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure S3 bucket\n",
    "\n",
    "bucket_name = \"spam-detector-storage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef18d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAM role, and bucket name prefix\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_key_prefix = \"sagemaker/spam-classifier\"\n",
    "\n",
    "vocabulary_length = 9013  #Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f104d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset\n",
    "\n",
    "os.system(\"mkdir -p dataset\")\n",
    "os.system(\n",
    "    \"curl https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip -o dataset/smsspamcollection.zip\"\n",
    ")\n",
    "os.system(\"unzip -o dataset/smsspamcollection.zip -d dataset\")\n",
    "os.system(\"head -10 dataset/SMSSpamCollection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944b0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to dataframe\n",
    "\n",
    "df = pd.read_csv(\"dataset/SMSSpamCollection\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5035f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the value Ham to 0 and Spam to 1 for better training\n",
    "\n",
    "df[df.columns[0]] = df[df.columns[0]].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f29a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent and dependent variables\n",
    "\n",
    "dependent = df[df.columns[0]].values\n",
    "independent = df[df.columns[1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85af8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the messages using One Hot Encoding\n",
    "\n",
    "one_hot_data = one_hot_encode(independent, vocabulary_length)\n",
    "encoded_messages = vectorize_sequences(one_hot_data, vocabulary_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f0a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign encoded messages and converted ham and spam details to main dataframe - reunion\n",
    "\n",
    "df2 = pd.DataFrame(encoded_messages)\n",
    "df2.insert(0, \"spam\", dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469e75c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing dataset ( 80% - training dataset, 20% - testing dataset )\n",
    "\n",
    "split_index = int(np.ceil(df.shape[0] * 0.8))\n",
    "training_set = df2[:split_index]\n",
    "validation_set = df2[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c597cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this dataframes to csv file for uploading these to S3\n",
    "\n",
    "training_set.to_csv(\"dataset/sms_train_set.gz\", header=False, index=False, compression=\"gzip\")\n",
    "validation_set.to_csv(\"dataset/sms_val_set.gz\", header=False, index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00faeaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='spam-detector-storage')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training and testing set into S3 storage for accessible - It takes some time to be uploaded\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "target_bucket = s3.Bucket(bucket_name)\n",
    "target_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e7276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "\n",
    "with open(\"dataset/sms_train_set.gz\", \"rb\") as data:\n",
    "    target_bucket.upload_fileobj(\n",
    "        data, \"{0}/train/sms_train_set.gz\".format(bucket_key_prefix)\n",
    "    )\n",
    "\n",
    "with open(\"dataset/sms_val_set.gz\", \"rb\") as data:\n",
    "    target_bucket.upload_fileobj(\n",
    "        data, \"{0}/val/sms_val_set.gz\".format(bucket_key_prefix)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ccd81cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "\r\n",
      "import pip\r\n",
      "\r\n",
      "try:\r\n",
      "    from pip import main as pipmain\r\n",
      "except:\r\n",
      "    from pip._internal import main as pipmain\r\n",
      "\r\n",
      "pipmain(['install', 'pandas'])\r\n",
      "import pandas\r\n",
      "\r\n",
      "#logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def train(hyperparameters, input_data_config, channel_input_dirs, output_data_dir,\r\n",
      "          num_gpus, num_cpus, hosts, current_host, **kwargs):\r\n",
      "    # SageMaker passes num_cpus, num_gpus and other args we can use to tailor training to\r\n",
      "    # the current container environment, but here we just use simple cpu context.\r\n",
      "    ctx = mx.cpu()\r\n",
      "\r\n",
      "    # retrieve the hyperparameters and apply some defaults in case they are not provided.\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.01)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 200)\r\n",
      "\r\n",
      "    train_data_path = channel_input_dirs['train']\r\n",
      "    val_data_path = channel_input_dirs['val']\r\n",
      "    train_data = get_train_data(train_data_path, batch_size)\r\n",
      "    val_data = get_val_data(val_data_path, batch_size)\r\n",
      "\r\n",
      "    # define the network\r\n",
      "    net = define_network()\r\n",
      "\r\n",
      "    # Collect all parameters from net and its children, then initialize them.\r\n",
      "    net.initialize(mx.init.Normal(sigma=1.), ctx=ctx)\r\n",
      "    \r\n",
      "    # Trainer is for updating parameters with gradient.\r\n",
      "    if len(hosts) == 1:\r\n",
      "        kvstore = 'device' if num_gpus > 0 else 'local'\r\n",
      "    else:\r\n",
      "        kvstore = 'dist_device_sync' if num_gpus > 0 else 'dist_sync'\r\n",
      "\r\n",
      "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\r\n",
      "                            {'learning_rate': learning_rate, 'momentum': momentum},\r\n",
      "                            kvstore=kvstore)\r\n",
      "    \r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\r\n",
      "\r\n",
      "    for epoch in range(epochs):\r\n",
      "        \r\n",
      "        # reset data iterator and metric at begining of epoch.\r\n",
      "        metric.reset()\r\n",
      "        btic = time.time()\r\n",
      "        for i, (data, label) in enumerate(train_data):\r\n",
      "            # Copy data to ctx if necessary\r\n",
      "            data = data.as_in_context(ctx)\r\n",
      "            label = label.as_in_context(ctx)\r\n",
      "            \r\n",
      "            # Start recording computation graph with record() section.\r\n",
      "            # Recorded graphs can then be differentiated with backward.\r\n",
      "            with autograd.record():\r\n",
      "                output = net(data)\r\n",
      "                L = loss(output, label)\r\n",
      "            L.backward()\r\n",
      "\r\n",
      "            # take a gradient step with batch_size equal to data.shape[0]\r\n",
      "            trainer.step(data.shape[0])\r\n",
      "\r\n",
      "            # update metric at last.\r\n",
      "            sigmoid_output = output.sigmoid() \r\n",
      "            prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "            metric.update([label], [prediction])\r\n",
      "\r\n",
      "            if i % log_interval == 0 and i > 0:\r\n",
      "                name, acc = metric.get()\r\n",
      "                print('[Epoch %d Batch %d] Training: %s=%f, %f samples/s' %\r\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\r\n",
      "\r\n",
      "            btic = time.time()\r\n",
      "\r\n",
      "        name, acc = metric.get()\r\n",
      "        print('[Epoch %d] Training: %s=%f' % (epoch, name, acc))\r\n",
      "\r\n",
      "        name, val_acc = test(ctx, net, val_data)\r\n",
      "        print('[Epoch %d] Validation: %s=%f' % (epoch, name, val_acc))\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    y = net(mx.sym.var('data'))\r\n",
      "    y.save('%s/model.json' % model_dir)\r\n",
      "    net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "def define_network():\r\n",
      "    net = nn.Sequential()\r\n",
      "    with net.name_scope():\r\n",
      "        net.add(nn.Dense(64, activation=\"relu\"))\r\n",
      "        net.add(nn.Dense(1))\r\n",
      "    return net\r\n",
      "\r\n",
      "def get_train_data(data_path, batch_size):\r\n",
      "    print('Train data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_train_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=True)\r\n",
      "\r\n",
      "def get_val_data(data_path, batch_size):\r\n",
      "    print('Validation data path: ' + data_path)\r\n",
      "    df = pandas.read_csv(data_path + '/sms_val_set.gz')\r\n",
      "    features = df[df.columns[1:]].values.astype(dtype=np.float32)\r\n",
      "    labels = df[df.columns[0]].values.reshape((-1, 1)).astype(dtype=np.float32)\r\n",
      "    \r\n",
      "    return gluon.data.DataLoader(gluon.data.ArrayDataset(features, labels), batch_size=batch_size, shuffle=False)\r\n",
      "\r\n",
      "def test(ctx, net, val_data):\r\n",
      "    metric = mx.metric.Accuracy()\r\n",
      "    for data, label in val_data:\r\n",
      "        data = data.as_in_context(ctx)\r\n",
      "        label = label.as_in_context(ctx)\r\n",
      "        \r\n",
      "        output = net(data)\r\n",
      "        sigmoid_output = output.sigmoid() \r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        metric.update([label], [prediction])\r\n",
      "    return metric.get()\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    net = gluon.nn.SymbolBlock(\r\n",
      "        outputs=mx.sym.load('%s/model.json' % model_dir),\r\n",
      "        inputs=mx.sym.var('data'))\r\n",
      "    \r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "\r\n",
      "    return net\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    try:\r\n",
      "        parsed = json.loads(data)\r\n",
      "        nda = mx.nd.array(parsed)\r\n",
      "        \r\n",
      "        output = net(nda)\r\n",
      "        sigmoid_output = output.sigmoid()\r\n",
      "        prediction = mx.nd.abs(mx.nd.ceil(sigmoid_output - 0.5))\r\n",
      "        \r\n",
      "        output_obj = {}\r\n",
      "        output_obj['predicted_label'] = prediction.asnumpy().tolist()\r\n",
      "        output_obj['predicted_probability'] = sigmoid_output.asnumpy().tolist()\r\n",
      "\r\n",
      "        response_body = json.dumps(output_obj)\r\n",
      "        return response_body, output_content_type\r\n",
      "    except Exception as ex:\r\n",
      "        response_body = '{error: }' + str(ex)\r\n",
      "        return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat \"latest_train_script.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "153febfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-03 05:41:06 Starting - Starting the training job...\n",
      "2021-12-03 05:41:31 Starting - Launching requested ML instancesProfilerReport-1638510066: InProgress\n",
      "......\n",
      "2021-12-03 05:42:31 Starting - Preparing the instances for training.........\n",
      "2021-12-03 05:43:56 Downloading - Downloading input data\n",
      "2021-12-03 05:43:56 Training - Downloading the training image..\u001b[34m2021-12-03 05:44:11,886 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[34m2021-12-03 05:44:11,886 INFO - root - starting train task\u001b[0m\n",
      "\u001b[34m2021-12-03 05:44:11,890 INFO - container_support.training - Training starting\u001b[0m\n",
      "\u001b[34m2021-12-03 05:44:12,963 WARNING - mxnet_container.train - #033[1;33mThis required structure for training scripts will be deprecated with the next major release of MXNet images. The train() function will no longer be required; instead the training script must be able to be run as a standalone script. For more information, see https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/mxnet#updating-your-mxnet-training-script.#033[1;0m\u001b[0m\n",
      "\u001b[34m2021-12-03 05:44:14,387 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'job_name': 'sms-spam-classifier-mxnet-2021-12-03-05-41-06-421', '_scheduler_ip': '10.0.224.118', 'user_script_name': 'latest_train_script.py', 'available_cpus': 8, 'user_requirements_file': None, '_ps_port': 8000, 'enable_cloudwatch_metrics': False, 'model_dir': '/opt/ml/model', 'available_gpus': 0, 'hyperparameters': {'sagemaker_job_name': 'sms-spam-classifier-mxnet-2021-12-03-05-41-06-421', 'learning_rate': 0.01, 'sagemaker_program': 'latest_train_script.py', 'sagemaker_container_log_level': 20, 'sagemaker_submit_directory': 's3://spam-detector-storage/sagemaker/spam-classifier/code/sms-spam-classifier-mxnet-2021-12-03-05-41-06-421/source/sourcedir.tar.gz', 'epochs': 20, 'batch_size': 100, 'sagemaker_region': 'us-east-1'}, 'channels': {'train': {'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated', 'TrainingInputMode': 'File'}, 'val': {'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated', 'TrainingInputMode': 'File'}}, 'output_data_dir': '/opt/ml/output/data/', 'input_dir': '/opt/ml/input', 'hosts': ['algo-1'], 'output_dir': '/opt/ml/output', 'base_dir': '/opt/ml', 'channel_dirs': {'train': '/opt/ml/input/data/train', 'val': '/opt/ml/input/data/val'}, 'container_log_level': 20, 'input_config_dir': '/opt/ml/input/config', '_ps_verbose': 0, 'resource_config': {'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}, 'current_host': 'algo-1', 'user_script_archive': 's3://spam-detector-storage/sagemaker/spam-classifier/code/sms-spam-classifier-mxnet-2021-12-03-05-41-06-421/source/sourcedir.tar.gz', '_scheduler_host': 'algo-1', 'code_dir': '/opt/ml/code', 'sagemaker_region': 'us-east-1'}\u001b[0m\n",
      "\u001b[34mDownloading s3://spam-detector-storage/sagemaker/spam-classifier/code/sms-spam-classifier-mxnet-2021-12-03-05-41-06-421/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[34m2021-12-03 05:44:14,659 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[34mCollecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/e3/d9f046b5d1c94a3aeab15f1f867aa414f8ee9d196fae6865f1d6a0ee1a0b/pytz-2021.3-py2.py3-none-any.whl (503kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.14.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, pandas\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-0.24.2 pytz-2021.3\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.5/dist-packages/mxnet_container/train.py:190: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  train_args = inspect.getargspec(user_module.train)\u001b[0m\n",
      "\u001b[34mTrain data path: /opt/ml/input/data/train\u001b[0m\n",
      "\n",
      "2021-12-03 05:44:32 Training - Training image download completed. Training in progress.\u001b[34mValidation data path: /opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34m[Epoch 0] Training: accuracy=0.727171\u001b[0m\n",
      "\u001b[34m[Epoch 0] Validation: accuracy=0.769093\u001b[0m\n",
      "\u001b[34m[Epoch 1] Training: accuracy=0.807943\u001b[0m\n",
      "\u001b[34m[Epoch 1] Validation: accuracy=0.830189\u001b[0m\n",
      "\u001b[34m[Epoch 2] Training: accuracy=0.856854\u001b[0m\n",
      "\u001b[34m[Epoch 2] Validation: accuracy=0.870620\u001b[0m\n",
      "\u001b[34m[Epoch 3] Training: accuracy=0.876150\u001b[0m\n",
      "\u001b[34m[Epoch 3] Validation: accuracy=0.862534\u001b[0m\n",
      "\u001b[34m[Epoch 4] Training: accuracy=0.887817\u001b[0m\n",
      "\u001b[34m[Epoch 4] Validation: accuracy=0.885894\u001b[0m\n",
      "\u001b[34m[Epoch 5] Training: accuracy=0.900157\u001b[0m\n",
      "\u001b[34m[Epoch 5] Validation: accuracy=0.884097\u001b[0m\n",
      "\u001b[34m[Epoch 6] Training: accuracy=0.911824\u001b[0m\n",
      "\u001b[34m[Epoch 6] Validation: accuracy=0.870620\u001b[0m\n",
      "\u001b[34m[Epoch 7] Training: accuracy=0.912946\u001b[0m\n",
      "\u001b[34m[Epoch 7] Validation: accuracy=0.908356\u001b[0m\n",
      "\u001b[34m[Epoch 8] Training: accuracy=0.920799\u001b[0m\n",
      "\u001b[34m[Epoch 8] Validation: accuracy=0.910153\u001b[0m\n",
      "\u001b[34m[Epoch 9] Training: accuracy=0.927978\u001b[0m\n",
      "\u001b[34m[Epoch 9] Validation: accuracy=0.906559\u001b[0m\n",
      "\u001b[34m[Epoch 10] Training: accuracy=0.927081\u001b[0m\n",
      "\u001b[34m[Epoch 10] Validation: accuracy=0.917341\u001b[0m\n",
      "\u001b[34m[Epoch 11] Training: accuracy=0.929998\u001b[0m\n",
      "\u001b[34m[Epoch 11] Validation: accuracy=0.924528\u001b[0m\n",
      "\u001b[34m[Epoch 12] Training: accuracy=0.930895\u001b[0m\n",
      "\u001b[34m[Epoch 12] Validation: accuracy=0.920036\u001b[0m\n",
      "\u001b[34m[Epoch 13] Training: accuracy=0.936953\u001b[0m\n",
      "\u001b[34m[Epoch 13] Validation: accuracy=0.920036\u001b[0m\n",
      "\u001b[34m[Epoch 14] Training: accuracy=0.936953\u001b[0m\n",
      "\u001b[34m[Epoch 14] Validation: accuracy=0.929919\u001b[0m\n",
      "\u001b[34m[Epoch 15] Training: accuracy=0.937177\u001b[0m\n",
      "\u001b[34m[Epoch 15] Validation: accuracy=0.932615\u001b[0m\n",
      "\u001b[34m[Epoch 16] Training: accuracy=0.938972\u001b[0m\n",
      "\u001b[34m[Epoch 16] Validation: accuracy=0.929919\u001b[0m\n",
      "\u001b[34m[Epoch 17] Training: accuracy=0.940319\u001b[0m\n",
      "\u001b[34m[Epoch 17] Validation: accuracy=0.926325\u001b[0m\n",
      "\u001b[34m[Epoch 18] Training: accuracy=0.945255\u001b[0m\n",
      "\n",
      "2021-12-03 05:45:06 Uploading - Uploading generated training model\n",
      "2021-12-03 05:45:06 Completed - Training job completed\n",
      "\u001b[34m[Epoch 18] Validation: accuracy=0.928122\u001b[0m\n",
      "\u001b[34m[Epoch 19] Training: accuracy=0.944582\u001b[0m\n",
      "\u001b[34m[Epoch 19] Validation: accuracy=0.929919\u001b[0m\n",
      "Training seconds: 82\n",
      "Billable seconds: 82\n"
     ]
    }
   ],
   "source": [
    "# Create the model \n",
    "\n",
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "output_path = 's3://{0}/{1}/output'.format(bucket_name, bucket_key_prefix)\n",
    "code_location = 's3://{0}/{1}/code'.format(bucket_name, bucket_key_prefix)\n",
    "\n",
    "m = MXNet('latest_train_script.py',\n",
    "          role=role,\n",
    "          train_instance_count=1,\n",
    "          instance_type='ml.c5.2xlarge',\n",
    "          output_path=output_path,\n",
    "          base_job_name='sms-spam-classifier-mxnet',\n",
    "          framework_version='1.2',\n",
    "          py_version='py3',\n",
    "          code_location = code_location,\n",
    "          hyperparameters={'batch_size': 100,\n",
    "                         'epochs': 20,\n",
    "                         'learning_rate': 0.01})\n",
    "\n",
    "inputs = {'train': 's3://{0}/{1}/train/'.format(bucket_name, bucket_key_prefix),\n",
    " 'val': 's3://{0}/{1}/val/'.format(bucket_name, bucket_key_prefix)}\n",
    "\n",
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934d4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------"
     ]
    }
   ],
   "source": [
    "# model deployment\n",
    "\n",
    "mxnet_pred = m.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.medium\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "85385e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: 1 validation error detected: Value '<endpoint_name>' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])*",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-6c1f5305b4e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mencoded_test_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_test_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_test_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmxnet_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_test_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: 1 validation error detected: Value '<endpoint_name>' at 'endpointName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9])*"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sagemaker.mxnet.model import MXNetPredictor\n",
    "\n",
    "# Uncomment the following line to connect to an existing endpoint.\n",
    "# mxnet_pred = MXNetPredictor('<endpoint_name>')\n",
    "\n",
    "test_messages = [\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\"]\n",
    "one_hot_test_messages = one_hot_encode(test_messages, vocabulary_length)\n",
    "encoded_test_messages = vectorize_sequences(one_hot_test_messages, vocabulary_length)\n",
    "print(encoded_test_messages)\n",
    "result = mxnet_pred.predict(encoded_test_messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "36a664c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<endpoint_name>\n"
     ]
    }
   ],
   "source": [
    "print(mxnet_pred.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bfe20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
